{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Understand Linear Regression modelling and apply it to a real dataset \n",
    "- Explore and analyse a real dataset Bikeshare Dataset to predict rental rate \n",
    "- Implement an ordinary linear regression model on a dataset satisfying linearity assumption using scikit-learn library.\n",
    "- Understand and identify multicollinearity in a multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Linear Regression\n",
    "A linear regression model predicts the value of a continuous target variable as a weighted sum of the input features. The target variable is denoted as $y$ and the input features are denoted as a feature vector $x$. It is assumed that $y$ is a linear function of one or more input features with some random noise.\n",
    "The relationship between each input features $x^{(1)}, x^{(2)}, ..., x^{(d)}$ and the target $y$ for one observation in the sample is:\n",
    "\\begin{equation}\n",
    "y=\\beta_{0}+\\beta_{1}x^{(1)}+\\ldots+\\beta_{d}x^{(d)}+\\epsilon\n",
    "\\end{equation}\n",
    "where $\\beta_{j}$ are the learned feature weights or coefficients, $x^{(j)}$ are the input features and $\\epsilon$ is the noise or error of the prediction. Each of the input features (e.g. $x^{(j)}$) has a highest degree of 1, therefore the equation represents a **linear** relationship between input and output target. A given set of weight values $\\beta_{j}$ gives a predicted output target which will then be compared against the actual label of that observation. A `loss` function $loss(\\hat{y}_i, y_i)$ is defined as the difference between the predicted target and the actual label for each training example $i$. It depends on which modeling problem we are trying to solve that we have different loss functions. In this linear regression problem we are going to use the Squared Error loss function which is the squared difference between predicted value and the actual label.\n",
    "\\begin{equation}\n",
    " loss = (\\hat{y}_{i} - y_{i})^{2}\n",
    "\\end{equation}\n",
    "The best weights or coefficients are the ones minimising the error or the difference between the predicted targets and the actual labels for all the training examples in the dataset (A side note: for now we use the training data as the ground for determining the best weights, we will revisit this defintion later on once we reach the definition of training error and test error). Therefore, we have to sum the loss over all training examples and calculate the Mean Squared Error (MSE) loss. This process of fitting the equation through all the data in the training examples is called training.\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{\\beta}}=\\arg\\!\\min_{\\beta_0,\\ldots,\\beta_p}\\sum_{i=1}^n(\\hat{y}_{i}- y_{i})^{2}\n",
    "\\end{equation}\n",
    "\n",
    "The above equation uses $argmin$ as a function to determine the set of $\\beta$s that minimise the overall MSE.\n",
    "\n",
    "The equation with the learned coefficients describe a line of best fit that minimises the difference between each actual points and the prediction values in a 2D space as in the below illustration. In a n-dimensional feature space, the equation describes a hyperplane.\n",
    "\n",
    "<center><img src='./assets/estimation.png'></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression simple 1-D example\n",
    "- This part illustrates a complete process on applying a machine learning model to a dataset. \n",
    "- Demonstrate the use of a simple linear regression model on a 1-dimensional toy dataset\n",
    "- Use common libraries such as numpy, pandas, seaborn and scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by collecting/ generating the data. Here we generate a random sample of 100 2-D points following a uniform distribution $\\mathcal{N}(0,1)$ where the x-y coordinates related linearly to each other. There is random noise (e.g $\\epsilon$) introduced into the data generation process to scatter the points out of an obvious straight line. This is also simulating the noise coming from our input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# visually examine the data\n",
    "\n",
    "plt.plot(X, y, 'b.')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.savefig(\"images/linear_01.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape), print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Machine Learning model in scikit-learn is quite straight-forward. We usually follow 2 processes: \n",
    "- instantiate an instance of the model, \n",
    "- then fit the model with the input-output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of a Linear Regression model from our sckitlearn library\n",
    "# and fit it through our data. The parameters are learned such that it minimises a loss function\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model finishes training, we use it to predict the target from an unseen data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients of the linear regression model can be accessed by the `coef_` and `intercept_` attributes of the model instance. In this example, since we only have 1-D data point, we can access $\\beta_0$ and $\\beta_1$ as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the coefficient Beta_0 and Beta_1 of the line\n",
    "# y = Beta_0 + x * Beta_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below plots show the training data points (in blue) and the new test data points (in green) together with the line described by the learned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X_new[0], y_predict[0], 'g*')\n",
    "plt.plot(X_new[1], y_predict[1], 'g*')\n",
    "plt.plot(X, y, 'b.')\n",
    "#plt.savefig(\"images/linear_02.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretation of the coefficients\n",
    "- The coefficients betas are considered as a weighting factor of how much each individual feature affects the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinear relationship between features and target variable\n",
    "We make a transformation on the original features to present a linear relationship between the target variable and the transformed features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 50\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to transform our features into polynomial format and run this transformation on different number of order `degree`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting vs. Underfitting the model on your data\n",
    "- Small degree of polynomial features may not reflect all the patterns and variation on your data, this leads to high training error.\n",
    "- High degree of polynomial features on the other hand may adapt to all the slight variations on your data, lead to very small training error but then it is hard to generalise to unseen data.\n",
    "- Generalisation is the ability of the machine learning model to predict well on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_overfitting(X, y, degree_list):\n",
    "    train_error = []\n",
    "    test_error = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for degree in degree_list:\n",
    "        polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "        linear_regression = LinearRegression()\n",
    "\n",
    "\n",
    "        X_train_poly = polynomial_features.fit_transform(X_train.reshape((-1,1)))\n",
    "        X_test_poly = polynomial_features.transform(X_test.reshape((-1,1)))\n",
    "\n",
    "\n",
    "        linear_regression.fit(X_train_poly, y_train)\n",
    "\n",
    "        y_train_pred = linear_regression.predict(X_train_poly)\n",
    "        y_test_pred = linear_regression.predict(X_test_poly)\n",
    "\n",
    "        train_error.append(mean_squared_error(y_train,y_train_pred))\n",
    "        test_error.append(mean_squared_error(y_test,y_test_pred))\n",
    "\n",
    "        print('MSE on train set:', mean_squared_error(y_train,y_train_pred))\n",
    "        print('MSE on validation set:', mean_squared_error(y_test,y_test_pred))\n",
    "        \n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_err, test_err = visualise_overfitting(X, y, [i for i in range(1, 30, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(1, 30, 2)], train_err, label = \"train\")\n",
    "plt.plot([i for i in range(1, 30, 2)], test_err, label = \"test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional topic on Regularised model\n",
    "# Using regularised model to avoid overfitting even with high degree polynomial order\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "polynomial_features = PolynomialFeatures(degree=10, include_bias=False)\n",
    "ridge_model = Ridge(alpha=0.00001, solver='cholesky') # alpha controls how much you want to regularise the model. alpha=0 is LinearRegression, larger value\n",
    "# of alpha will compress the coefficients to be small and lead to a straight line (extremely underfit)\n",
    "\n",
    "X_test = np.linspace(0, 1, 100)\n",
    "\n",
    "X_train_poly = polynomial_features.fit_transform(X.reshape((-1,1)))\n",
    "X_test_poly = polynomial_features.transform(X_test.reshape((-1,1)))\n",
    "\n",
    "#print(X_poly)\n",
    "# training the model on traning set\n",
    "ridge_model.fit(X_train_poly, y)\n",
    "\n",
    "\n",
    "y_pred = ridge_model.predict(X_test_poly)\n",
    "\n",
    "\n",
    "plt.plot(X_test, y_pred, label=\"Model\")\n",
    "plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-2, 2))\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Evaluation Metrics:\n",
    "\n",
    "With $n$ is the number of training examples, we have the following error metrics:\n",
    "\n",
    "**Mean absolute error (MAE)** is the mean of the absolute value of the errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "- Easy to understand\n",
    "- Does not highlight the effect of outliers\n",
    "\n",
    "**Mean squared error (MSE)** is the mean of the squared errors:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "- The squared part is used to make derivative easier to deal with in some optimization algorithms\n",
    "- Highlight the significance of outliers\n",
    "\n",
    "**Root mean squared error (RMSE)** is the square root of the mean of the squared errors:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "- Give the benefit that the errors' unit is the same as of the targets\n",
    "- Still maintain the squared to highlight the significance of outliers\n",
    "\n",
    "In scikit-learn:\n",
    "```\n",
    ">>> print('MAE:', metrics.mean_absolute_error(true, pred))\n",
    ">>> print('MSE:', metrics.mean_squared_error(true, pred))\n",
    ">>> print('RMSE:', np.sqrt(metrics.mean_squared_error(true, pred)))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null RMSE is the RMSE that could be achieved by always predicting the mean response value. It is a benchmark against which you may want to measure your regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Stochastic Gradient Descent Regressor\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
